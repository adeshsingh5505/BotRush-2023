{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeshsingh5505/BotRush-2023/blob/main/webScrapping_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CpjaW4Gxchc6"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v4r21vz6cyTV"
      },
      "outputs": [],
      "source": [
        "excel_file = 'Input.xlsx'\n",
        "df = pd.read_excel(excel_file)\n",
        "urls = df['URL'].tolist()\n",
        "ids = df['URL_ID'].tolist()\n",
        "for url, url_id in zip(urls, ids):\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "\n",
        "    # Extracting title and body\n",
        "    title = soup.title.text if soup.title else 'No title found'\n",
        "    body2=soup.find(class_=\"td-post-content tagdiv-type\")\n",
        "    body=body2.get_text()\n",
        "\n",
        "\n",
        "    content = f\"Title: {title}\\n\\nBody:\\n{body}\"\n",
        "\n",
        "    # saving content under URL_ID\n",
        "    file_name = f\"{url_id}.txt\"\n",
        "    with open(file_name, 'w', encoding='utf-8') as file:\n",
        "        file.write(content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kLkBNevgcySF"
      },
      "outputs": [],
      "source": [
        "# pushing evry file starting with bctech in folder\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_bctech_files(src_dir, dest_dir):\n",
        "    # Ensuring destination directory exists\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "\n",
        "    for filename in os.listdir(src_dir):\n",
        "\n",
        "        if filename.startswith('bctech') and filename.endswith('.txt'):\n",
        "            # Constructing full file path\n",
        "            src_file = os.path.join(src_dir, filename)\n",
        "            dest_file = os.path.join(dest_dir, filename)\n",
        "\n",
        "            # Moving the file\n",
        "            shutil.move(src_file, dest_file)\n",
        "            print(f\"Moved: {filename}\")\n",
        "\n",
        "source_directory = '/content'  #source path\n",
        "destination_directory = '/content/Analysis'  #  destination path\n",
        "\n",
        "move_bctech_files(source_directory, destination_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PrtBGQrrD04"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kO9KLq5K9CMq"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "stopWords='/content/StopWords-20240711T223622Z-001.zip'\n",
        "with ZipFile(stopWords,'r') as zip:\n",
        "  zip.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install chardet"
      ],
      "metadata": {
        "id": "yCB-4KMPvKAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_atIYkYrIQM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "# Downloading NLTK files\n",
        "nltk.download('punkt')\n",
        "\n",
        "def extract_zip(zip_file_path, extract_to_folder):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_folder)\n",
        "\n",
        "def load_stopwords(stopwords_folder):\n",
        "    stop_words = set()\n",
        "    for filename in os.listdir(stopwords_folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(stopwords_folder, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                stop_words.update(word.strip().lower() for word in file.readlines())\n",
        "    return stop_words\n",
        "\n",
        "def clean_text(text, stop_words):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    cleaned_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "def create_sentiment_dict():\n",
        "    # Define positive and negative words (example lists, you can expand these)\n",
        "    positive_words = {'happy', 'good', 'great', 'excellent', 'positive', 'fortunate', 'correct', 'superior'}\n",
        "    negative_words = {'sad', 'bad', 'terrible', 'awful', 'negative', 'unfortunate', 'wrong', 'inferior'}\n",
        "\n",
        "    sentiment_dict = defaultdict(lambda: 'neutral')\n",
        "    for word in positive_words:\n",
        "        sentiment_dict[word] = 'positive'\n",
        "    for word in negative_words:\n",
        "        sentiment_dict[word] = 'negative'\n",
        "\n",
        "    return sentiment_dict\n",
        "\n",
        "def extract_derived_variables(tokens, sentiment_dict):\n",
        "    positive_count = 0\n",
        "    negative_count = 0\n",
        "\n",
        "    for token in tokens:\n",
        "        if sentiment_dict[token] == 'positive':\n",
        "            positive_count += 1\n",
        "        elif sentiment_dict[token] == 'negative':\n",
        "            negative_count += 1\n",
        "\n",
        "    return positive_count, negative_count\n",
        "\n",
        "def read_file_with_multiple_encodings(file_path):\n",
        "    encodings = ['utf-8', 'latin-1', 'windows-1252']  # List of encodings to try\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding=encoding) as file:\n",
        "                return file.read()\n",
        "        except UnicodeDecodeError:\n",
        "            continue\n",
        "    raise UnicodeDecodeError(f\"Unable to decode {file_path} with tried encodings\")\n",
        "\n",
        "def analyze_text_files(input_folder, output_excel_file, stopwords_folder):\n",
        "    # Load stop words\n",
        "    stop_words = load_stopwords(stopwords_folder)\n",
        "\n",
        "    sentiment_dict = create_sentiment_dict()\n",
        "\n",
        "    results = defaultdict(dict)\n",
        "\n",
        "    # Iterate over all text files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith('.txt'):\n",
        "            input_file_path = os.path.join(input_folder, filename)\n",
        "            url_id = filename.rsplit('.', 1)[0]  # Extract url_id from filename (assumes filenames are like url_id.txt)\n",
        "\n",
        "            # Reading stopwords\n",
        "            try:\n",
        "                text = read_file_with_multiple_encodings(input_file_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "            cleaned_tokens = clean_text(text, stop_words)\n",
        "\n",
        "            # Extract derived variables\n",
        "            positive_count, negative_count = extract_derived_variables(cleaned_tokens, sentiment_dict)\n",
        "\n",
        "\n",
        "            results['url_id'][url_id] = url_id\n",
        "            results['Total Words'][url_id] = len(cleaned_tokens)\n",
        "            results['Positive Words'][url_id] = positive_count\n",
        "            results['Negative Words'][url_id] = negative_count\n",
        "\n",
        "\n",
        "            print(f\"Processed {filename}: Total Words={len(cleaned_tokens)}, Positive Words={positive_count}, Negative Words={negative_count}\")\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "\n",
        "    print(\"DataFrame Content:\\n\", df)\n",
        "\n",
        "    # Saving to excel\n",
        "    os.makedirs(os.path.dirname(output_excel_file), exist_ok=True)\n",
        "    df.to_excel(output_excel_file, index=False)\n",
        "\n",
        "    # Debug print to confirm saving\n",
        "    print(f\"Results saved to {output_excel_file}\")\n",
        "\n",
        "# Example usage\n",
        "input_folder = '/content/Analysis'  # Replace with your input folder path\n",
        "output_excel_file = '/content/Output Data Structure.xlsx'  # Replace with your output Excel file path\n",
        "extracted_stopwords_folder = '/mnt/data/StopWords_extracted'  # Temporary folder for extracted stopwords\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "zip_file_path = '/content/stopwords.zip'\n",
        "extract_zip(zip_file_path, extracted_stopwords_folder)\n",
        "\n",
        "# Run the analysis\n",
        "analyze_text_files(input_folder, output_excel_file, extracted_stopwords_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiVD8wkYr6ez"
      },
      "outputs": [],
      "source": [
        "pip install nltk textstat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "PQ2bmSR1r-O1",
        "outputId": "6824354c-ba8b-4d84-bcbf-2eb3a48de048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-7fc23c46051e>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0murl_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract url_id from filename (assumes filenames are like url_id.txt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0minput_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0manalysis_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_text_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalysis_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-7fc23c46051e>\u001b[0m in \u001b[0;36manalyze_text_from_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0manalyze_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-7fc23c46051e>\u001b[0m in \u001b[0;36manalyze_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnum_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mnum_complex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcount_syllables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtotal_syllables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_syllables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mavg_words_per_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_sentences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_sentences\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-7fc23c46051e>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnum_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mnum_complex_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcount_syllables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mtotal_syllables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_syllables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mavg_words_per_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_words\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_sentences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_sentences\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-7fc23c46051e>\u001b[0m in \u001b[0;36mcount_syllables\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Function to count syllables in a word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcount_syllables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmudict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/cmudict.py\u001b[0m in \u001b[0;36mdict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mlowercase\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwhose\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mlists\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpronunciations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pairs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mdefaultdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/util.py\u001b[0m in \u001b[0;36miterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[1;32m    308\u001b[0m                 \u001b[0;34m\"block reader %s() should return list or tuple.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/cmudict.py\u001b[0m in \u001b[0;36mread_cmudict_block\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mentries\u001b[0m  \u001b[0;31m# end of file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mpieces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mentries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpieces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpieces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import cmudict\n",
        "from textstat import textstat\n",
        "\n",
        "# Ensure necessary resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "\n",
        "# Function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    d = cmudict.dict()\n",
        "    if word.lower() in d:\n",
        "        return len([y for y in d[word.lower()][0] if y[-1].isdigit()])\n",
        "    else:\n",
        "        return textstat.syllable_count(word)\n",
        "\n",
        "# Function to calculate readability and other metrics\n",
        "def analyze_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    # Count personal pronouns\n",
        "    personal_pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.I)\n",
        "\n",
        "    # Calculate metrics\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(words)\n",
        "    num_complex_words = sum(1 for word in words if count_syllables(word) >= 3)\n",
        "    total_syllables = sum(count_syllables(word) for word in words)\n",
        "    avg_words_per_sentence = num_words / num_sentences if num_sentences > 0 else 0\n",
        "    avg_syllables_per_word = total_syllables / num_words if num_words > 0 else 0\n",
        "    avg_word_length = sum(len(word) for word in words) / num_words if num_words > 0 else 0\n",
        "\n",
        "    # Analysis\n",
        "    readability = textstat.flesch_reading_ease(text)\n",
        "\n",
        "    return {\n",
        "        \"Readability Score\": readability,\n",
        "        \"Average Number of Words Per Sentence\": avg_words_per_sentence,\n",
        "        \"Complex Word Count\": num_complex_words,\n",
        "        \"Word Count\": num_words,\n",
        "        \"Syllable Count Per Word\": avg_syllables_per_word,\n",
        "        \"Personal Pronouns\": len(personal_pronouns),\n",
        "        \"Average Word Length\": avg_word_length\n",
        "    }\n",
        "\n",
        "# Function to read text from a file and analyze it\n",
        "def analyze_text_from_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    return analyze_text(text)\n",
        "\n",
        "# Example usage\n",
        "input_folder = '/content/Analysis'  # Replace with your input folder path\n",
        "output_excel_file = '/content/Output Data Structure.xlsx'  # Replace with your output Excel file path\n",
        "\n",
        "results = {}\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.txt'):\n",
        "        url_id = filename.rsplit('.', 1)[0]  # Extract url_id from filename (assumes filenames are like url_id.txt)\n",
        "        input_file_path = os.path.join(input_folder, filename)\n",
        "        analysis_result = analyze_text_from_file(input_file_path)\n",
        "        results[url_id] = analysis_result\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df = pd.DataFrame(results).T\n",
        "\n",
        "# Save to Excel\n",
        "os.makedirs(os.path.dirname(output_excel_file), exist_ok=True)\n",
        "df.to_excel(output_excel_file, index_label='url_id')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz4OiVI+T2NSGaVim68lWE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}